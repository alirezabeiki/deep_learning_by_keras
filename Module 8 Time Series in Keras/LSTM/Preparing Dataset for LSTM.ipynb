{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing datasets for LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to every LSTM layer must be three-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three dimensions of this input are:\n",
    "\n",
    "- <font color=blue> __Samples__:</font>    One sequence is one sample. A batch is comprised of one or more samples.\n",
    "- <font color=blue> __Time Steps:__</font> One time step is one point of observation in the sample.\n",
    "- <font color=blue> __Features:__</font>   One feature is one observation at a time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the input layer expects a 3D array of data when fitting the model and when making predictions, even if specific dimensions of the array contain a single value, e.g. one sample or one feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining the input layer of your LSTM network, the network assumes you have 1 or more samples and requires that you specify the number of time steps and the number of features. You can do this by specifying a tuple to the “input_shape” argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the model below defines an input layer that expects 1 or more samples, 50 time steps, and 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(50, 2)))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to define an LSTM input layer and the expectations of 3D inputs, let’s look at some examples of how we can prepare our data for the LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> __Notice__:</font>\n",
    "\n",
    "- LSTMs expect 3D input, and it can be challenging.\n",
    "- LSTMs don’t like sequences of more than 200-400 time steps, so the data will need to be split into samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of LSTM With Single Input Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case where you have one sequence of multiple time steps and one feature.\n",
    "\n",
    "For example, this could be a sequence of 10 values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\n",
    "\n",
    "We can define this sequence of numbers as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the reshape() function on the NumPy array to reshape this one-dimensional array into a three-dimensional array with 1 sample, 10 time steps, and 1 feature at each time step.\n",
    "\n",
    "The reshape() function when called on an array takes one argument which is a tuple defining the new shape of the array. We cannot pass in any tuple of numbers; the reshape must evenly reorganize the data in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reshape((1, 10, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once reshaped, we can print the new shape of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all of this together, the complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "data = data.reshape((1, 10, 1))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is now ready to be used as input (X) to the LSTM with an input_shape of (10, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(10, 1)))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of LSTM with Multiple Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case where you have multiple parallel series as input for your model.\n",
    "\n",
    "For example, this could be two parallel series of 10 values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> series 1:</font> 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\n",
    "\n",
    "<font color=red> series 2:</font> 1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define these data as a matrix of 2 columns with 10 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "data = array([\n",
    "\t[0.1, 1.0],\n",
    "\t[0.2, 0.9],\n",
    "\t[0.3, 0.8],\n",
    "\t[0.4, 0.7],\n",
    "\t[0.5, 0.6],\n",
    "\t[0.6, 0.5],\n",
    "\t[0.7, 0.4],\n",
    "\t[0.8, 0.3],\n",
    "\t[0.9, 0.2],\n",
    "\t[1.0, 0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data can be framed as 1 sample with 10 time steps and 2 features.\n",
    "\n",
    "It can be reshaped as a 3D array as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reshape(1, 10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all of this together, the complete example is listed below and running the example prints the new 3D shape of the single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "data = array([\n",
    "\t[0.1, 1.0],\n",
    "\t[0.2, 0.9],\n",
    "\t[0.3, 0.8],\n",
    "\t[0.4, 0.7],\n",
    "\t[0.5, 0.6],\n",
    "\t[0.6, 0.5],\n",
    "\t[0.7, 0.4],\n",
    "\t[0.8, 0.3],\n",
    "\t[0.9, 0.2],\n",
    "\t[1.0, 0.1]])\n",
    "data = data.reshape(1, 10, 2)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is now ready to be used as input (X) to the LSTM with an input_shape of (10, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(10, 2)))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for LSTM Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section lists some tips to help you when preparing your input data for LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The LSTM input layer must be 3D.\n",
    "- The meaning of the 3 input dimensions are: samples, time steps, and features.\n",
    "- The LSTM input layer is defined by the input_shape argument on the first hidden layer.\n",
    "- The input_shape argument takes a tuple of two values that define the number of time steps and features.\n",
    "- The number of samples is assumed to be 1 or more.\n",
    "- The reshape() function on NumPy arrays can be used to reshape your 1D or 2D data to be 3D.\n",
    "- The reshape() function takes a tuple as an argument that defines the new shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques to Handle Very Long Sequences with LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short-Term Memory or LSTM recurrent neural networks are capable of learning and remembering over long sequences of inputs.\n",
    "\n",
    "LSTMs work very well if your problem has one output for every input, like time series forecasting or text translation. But LSTMs can be challenging to use when you have very long input sequences and only one or a handful of outputs.\n",
    "\n",
    "This is often called sequence labeling, or sequence classification.\n",
    "\n",
    "Some examples include:\n",
    "\n",
    "- Classification of sentiment in documents containing thousands of words (natural language processing).\n",
    "- Classification of an EEG trace of thousands of time steps (medicine).\n",
    "- Classification of coding or non-coding genes for sequences of thousands of DNA base pairs (bioinformatics).\n",
    "\n",
    "These so-called sequence classification tasks require special handling when using recurrent neural networks, like LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use Sequences As-Is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starting point is to use the long sequence data as-is without change.\n",
    "\n",
    "This may result in the problem of very long training times.\n",
    "\n",
    "More troubling, attempting to back-propagate across very long input sequences may result in vanishing gradients, and in turn, an unlearnable model.\n",
    "\n",
    "A reasonable limit of 250-500 time steps is often used in practice with large LSTM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Truncate Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common technique for handling very long sequences is to simply truncate them.\n",
    "\n",
    "This can be done by selectively removing time steps from the beginning or the end of input sequences.\n",
    "\n",
    "This will allow you to force the sequences to a manageable length at the cost of losing data.\n",
    "\n",
    "The risk of truncating input sequences is that data that is valuable to the model in order to make accurate predictions is being lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summarize Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some problem domains, it may be possible to summarize the input sequences.\n",
    "\n",
    "For example, in the case where input sequences are words, it may be possible to remove all words from input sequences that are above a specified word frequency (e.g. “and”, “the”, etc.).\n",
    "\n",
    "This could be framed as only keep the observations where their ranked frequency in the entire training dataset is above some fixed value.\n",
    "\n",
    "Summarization may result in both focusing the problem on the most salient parts of the input sequences and sufficiently reducing the length of input sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A less systematic approach may be to summarize a sequence using random sampling.\n",
    "\n",
    "Random time steps may be selected and removed from the sequence in order to reduce them to a specific length.\n",
    "\n",
    "Alternately, random contiguous subsequences may be selected to construct a new sampled sequence over the desired length, care to handle overlap or non-overlap as required by the domain.\n",
    "\n",
    "This approach may be suitable in cases where there is no obvious way to systematically reduce the sequence length.\n",
    "\n",
    "This approach may also be used as a type of data augmentation scheme in order to create many possible different input sequences from each input sequence. Such methods can improve the robustness of models when available training data is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use Truncated Backpropagation Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than updating the model based on the entire sequence, the gradient can be estimated from a subset of the last time steps.\n",
    "\n",
    "This is called Truncated Backpropagation Through Time, or TBPTT for short. It can dramatically speed up the learning process of recurrent neural networks like LSTMs on long sequences.\n",
    "\n",
    "This would allow all sequences to be provided as input and execute the forward pass, but only the last tens or hundreds of time steps would be used to estimate the gradients and used in weight updates.\n",
    "\n",
    "Some modern implementations of LSTMs permit you to specify the number of time steps to use for updates, separate for the time steps used as input sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use an Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use an autoencoder to learn a new representation length for long sequences, then a decoder network to interpret the encoded representation into the desired output.\n",
    "\n",
    "This may involve an unsupervised autoencoder as a pre-processing pass on sequences, or the more recent encoder-decoder LSTM style networks used for natural language translation.\n",
    "\n",
    "Again, there may still be difficulties in learning from very long sequences, but the more sophisticated architecture may offer additional leverage or skill, especially if combined with one or more of the techniques above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume you know how to load the data as a Pandas Series or DataFrame.\n",
    "\n",
    "Here, we will mock loading by defining a new dataset in memory with 5,000 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 10]\n",
      " [ 2 20]\n",
      " [ 3 30]\n",
      " [ 4 40]\n",
      " [ 5 50]]\n",
      "(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "\n",
    "# load...\n",
    "data = list()\n",
    "n = 5000\n",
    "for i in range(n):\n",
    "\tdata.append([i+1, (i+1)*10])\n",
    "data = array(data)\n",
    "print(data[:5, :])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this piece both prints the first 5 rows of data and the shape of the loaded data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Drop Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your time series data is uniform over time and there is no missing values, we can drop the time column.\n",
    "\n",
    "If not, you may want to look at imputing the missing values, resampling the data to a new time scale, or developing a model that can handle missing values. See posts like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we just drop the first column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "# drop time\n",
    "data = data[:, 1]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an array of 5,000 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Into Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs need to process samples where each sample is a single time series.\n",
    "\n",
    "In this case, 5,000 time steps is too long; LSTMs work better with 200-to-400 time steps based on some papers I’ve read. Therefore, we need to split the 5,000 time steps into multiple shorter sub-sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will split the 5,000 time steps into 25 sub-sequences of 200 time steps each. Rather than using NumPy or Python tricks, we will do this the old fashioned way so you can see what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "# split into samples (e.g. 5000/200 = 25)\n",
    "samples = list()\n",
    "length = 200\n",
    "# step over the 5,000 in jumps of 200\n",
    "for i in range(0,n,length):\n",
    "\t# grab from i to i + 200\n",
    "\tsample = data[i:i+length]\n",
    "\tsamples.append(sample)\n",
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 25 sub sequences of 200 time steps each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reshape Subsequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM needs data with the format of [samples, time steps and features]. Here, we have 25 samples, 200 time steps per sample, and 1 feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to convert our list of arrays into a 2D NumPy array of 25 x 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 200)\n"
     ]
    }
   ],
   "source": [
    "# convert list of arrays into 2d array\n",
    "data = array(samples)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the reshape() function to add one additional dimension for our single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape into [samples, timesteps, features]\n",
    "# expect [25, 200, 1]\n",
    "data = data.reshape((len(samples), length, 1))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1. How to Reshape Input Data for Long Short-Term Memory Networks in Keras ([Link](https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/))\n",
    "1. Techniques to Handle Very Long Sequences with LSTMs ([Link](https://machinelearningmastery.com/handle-long-sequences-long-short-term-memory-recurrent-neural-networks/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How to Convert a Time Series to a Supervised Learning Problem in Python ([Link](https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/))\n",
    "2. Time Series Forecasting as Supervised Learning ([Link](https://machinelearningmastery.com/time-series-forecasting-supervised-learning/))\n",
    "3. How to Load and Explore Time Series Data in Python ([Link](https://machinelearningmastery.com/load-explore-time-series-data-python/))\n",
    "4. How to Load and Explore Time Series Data in Python ([Link](https://machinelearningmastery.com/load-explore-time-series-data-python/))\n",
    "5. How To Load Machine Learning Data in Python ([Link](https://machinelearningmastery.com/load-machine-learning-data-python/))\n",
    "6. How to Develop LSTM Models for Time Series Forecasting ([Link](https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
